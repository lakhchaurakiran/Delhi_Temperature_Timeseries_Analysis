---
title: "Delhi temperature timeseries data SARIMA model fitting"
author: "Kiran Lakhchaura"
date: "2/3/2021"
output:
  pdf_document: default
  html_document: default
  fig_width: 6 
  fig_height: 4 
---

In this project I will be doing a time-series analysis of the daily temperature in the city of Delhi for the period of Jan 01, 1995 to Dec 31, 2019.

```{r include = FALSE}
library(zoo)
library(tidyr)
library(dplyr)
library(lubridate)
library(astsa)
library(forecast)
library(tseries)
```

\
**Reading the data**\


```{r}
DelhiTemp <- read.csv(file = '../Data/Delhi_temperature_1995_2020.csv')
```

\
**Viewing the data**\


```{r}
head(DelhiTemp)
```
\
Creating a date column and a YearMon column from Month, Day and Year columns.\


```{r}
DelhiTemp$Date <- as.Date(with(DelhiTemp, paste(Year, Month, Day,sep="-")), "%Y-%m-%d")
DelhiTemp$YearMon <- as.yearmon(paste(DelhiTemp$Year, DelhiTemp$Month), "%Y %m")

```

\
**Plotting the data**\
Temperature vs. Date plot\

```{r, fig.height=5, fig.width=15}
Temp <- DelhiTemp$Temperature
Date <- DelhiTemp$Date
plot(Temp~Date)
```
\
Our data shows clear yearly seasonality and their are some very large outliers (temperature~-100) possibly because of missing data for those dates.

### Outlier detection and missing value treatment

Identifying outliers and replacing them with backfilled values:\

```{r}
DelhiTemp$Temperature[DelhiTemp$Temperature < 0] <- NA
which(is.na(DelhiTemp$Temperature))
DelhiTemp$Temperature <- na.locf(DelhiTemp$Temperature, fromLast = TRUE)
which(is.na(DelhiTemp$Temperature))
```
\
**Updated Temperature vs. Date plot**\

```{r,fig.height=5, fig.width=15}
Temp <- DelhiTemp$Temperature
Date <- DelhiTemp$Date
plot(Temp~Date)
```
\
### Binning

Binning temperatures to their mean monthly values\

```{r}
DelhiMonthlyTemp <- DelhiTemp %>% group_by("YearMon"=DelhiTemp$YearMon) %>% summarize(Temperature = mean(Temperature))

head(DelhiMonthlyTemp);
```
\
**Monthly mean temperature vs. Date (Month) plot**\

```{r, fig.height=5, fig.width=15}
Temp <- DelhiMonthlyTemp$Temperature
Date <- DelhiMonthlyTemp$YearMon
plot(Temp~Date,type="l")
```
\
Once again we see that the data shows clear seasonality but no variation in variance so differencing should be enough for taking care of the trend.

### Stationarity tests

Checking for stationarity using ADF test\

```{r}
adf.test(Temp)
```
\
ADF test p-value (<0.05) suggests that the null-hypothesis of unit-root (non-statinarity) should be rejected => data is stationary.

Checking for trend stationarity using KPSS test\

```{r}
kpss.test(Temp, null=c("Trend"), lshort=TRUE)
```
\
KPSS p-value (>0.05) indicates that the null hypothesis of trend stationarity cannot be rejected.

Let's do the PP unit-root test now\

```{r}
pp.test(Temp,lshort = TRUE)
```
\
The PP unit root test p-value (<0.05) indicates that the null hypothesis of unit-root (non-stationarity) should be rejected => data is stationary.

So all three tests indicate that the data is stationary.

Now let's look at the auto-correlations in the ACF and PACF plots.

### Auto-Correlation Functions

Let's first look at the auto-correlation function and the partial auto-correlation function plots.\

```{r, fig.height=5, fig.width=15}
par(mfrow=c(2,1))
acf(Temp,main="Auto-Correlation Function of Delhi's mean monthly temperatures",50)
pacf(Temp,main="Partial Auto-Correlation Function of Delhi's mean monthly temperatures",50);
```
\
We see that the ACF plot also shows lots of correlation and clear seasonality.

## Guessing the right orders for (S)ARIMA model fitting

### 1. Differncing orders (d, D)

Since we do see clear seasonality and trend in the data, we should look at the differenced data using both seasonal as well as non-seasonal differencing -> *diff(diff(data),12)*\

```{r, fig.height=5, fig.width=15}
plot(diff(diff(Temp),12),type="l",main="differenced data -> diff(diff(data),12)")
```
\
the plot shows almost no-trends except for a few large peaks at the center which may be outliers => d=1, D=1. We can also use the ADF test for checking the stationarity\

```{r}
diff_data <- diff(diff(Temp),12)
adf.test(diff_data)
```
\
The test confirms that the differenced data is stationary.

### 2. Finding the best orders for the auro-regressive (AR; p, P) and Moving Average (MA; q, Q) terms

**ACF and PACF for differenced data**\

```{r, fig.height=5, fig.width=15}
par(mfrow=c(2,1))
acf(diff_data,main='differnced data ACF',50)
pacf(diff_data,main='differnced data PACF',50);
```
\
The ACF plot shows significant correlations at lag=1,11 and 12 while the PACF shows significant correlation for lag=1,11 and 12 The correlations at later parts may be due to seasonality

### Finding best parameters using

1. Grid Search

Trying for different values of p,q,P,Q and note down AIC, SSE and p-value (for Ljun-box-test). 
We want high p-values and small AIC and SSE using parsimony principle (simpler the better) while searching\

```{r}
d=1; DD=1; per=12

for(p in 1:2){
  for(q in 1:2){
    for(i in 1:6){
      for(j in 1:3){
        if(p+d+q+i+DD+j<=10){
          
          model<-arima(x=Temp, order = c((p-1),d,(q-1)), seasonal = list(order=c((i-1),DD,(j-1)), period=per))
          
          pval<-Box.test(model$residuals, lag=log(length(model$residuals)))
          
          sse<-sum(model$residuals^2)
          
          cat(p-1,d,q-1,i-1,DD,j-1,per, 'AIC=', model$aic, ' SSE=',sse,' p-VALUE=', pval$p.value,'\n')
          
        }
      }
    }
  }
}
```
\
2. auto.arima()\

```{r}
y <- msts(Temp, seasonal.periods=c(12))
auto.arima( y, d = 1, D = 1,  max.p = 5,  max.q = 5,  max.P = 5,  max.Q = 5, max.order = 10,  start.p = 1,  start.q = 1,  start.P = 0, start.Q = 0, stationary = FALSE, seasonal = TRUE, ic="aic", stepwise = TRUE, approximation = FALSE)
```


## Best-model

For some reason auto-arima is unable to reproduce the minimum value of AIC which was found in the grid-search method. From the grid-search the lowest AIC of 1221.998 is found for a 1,1,1,0,1,1,12 SARIMA model which also has a large enough p-value.


## Fitting the best-model on the data

### Train-test split\

```{r}
N = length(Temp)
n = round(0.7*N)
train = Temp[1:n]
test  = Temp[(n+1):N]
```

### Training the model with the train set\

```{r, fig.height=5, fig.width=15}
model<-arima(x=train, order = c(1,1,1), seasonal = list(order=c(0,1,1), period=per))
standard_residuals<- model$residuals/sd(model$residuals)
plot(standard_residuals,ylab='',main='Standardized Residuals')
```
\
We see that the residuals look almost stationary which we also confirmed with the ADF test\

```{r}
print(adf.test(standard_residuals))
```
\
Let's check for correlations in the residual using the ACF plot\

```{r, fig.height=5, fig.width=15}
acf(standard_residuals,50,main='ACF of standardized residuals');
```
\
Next, we will perform a Ljung-Box test on the residuals. The null hypotheis for the test is:\
H0: The dataset points are independently distributed (not correlated).\
where a p-value of greater than 0.05 will be insifficient to reject the null hypothesis.\

```{r, fig.height=5, fig.width=15}
for (lag in seq(1:50)){
  pval<-Box.test(model$residuals, lag=lag)
  p[lag]=pval$p.value
}
plot(p,ylim = (0.0:1), main='p-value from Ljung-Box test')
abline(h=0.05,lty=2)
```
\
Any value above the dashed line (at y=0.05) is significant. We see that the p-values of the Ljung-Box test at all the lags are very significant and therefore the hypothesis that the residuals are not correlated cannot be rejected.

### Testing the predictions on the test set\

```{r, fig.height=5, fig.width=15}
model<-arima(x=train, order = c(1,1,1), seasonal = list(order=c(0,1,1), period=per))
pred_len=length(test)
plot(forecast(model, h=pred_len),main='Testing predictions')
train_x = seq(length(train)+1,length(train)+length(test))
lines(train_x,test)
```
\
Here the black lines in the first part (left) shows the training data and those in the second part shows the test data which alos has blue lines overlaid on it showing the predictions from our model which seem to match the test data pretty well. The small shaded region on the blue lines shows the confidence interval (difficult to resolve here but it actually consists of two different dark and light shaded regions showing the 80% and 95% confidence regions).

## Evaluating predictions\

```{r}
df2 <- forecast(model,h=pred_len)
df2 <- data.frame(df2)
print(paste0('Root Mean Squared Error in predictions =', round(sqrt(mean((test-df2[,1])**2))/mean(test)*100,2), '%'))
```


## Forecasting using the best-model\

```{r, fig.height=5, fig.width=15}
model<-arima(x=Temp, order = c(1,1,1), seasonal = list(order=c(0,1,1), period=per))
par(mfrow=c(1,1))
h=12 # forecasting for the 12 months after the end of the dataset
plot(forecast(model,h), main='Forecasts for next 12 months'); 
```
